{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree"
      ],
      "metadata": {
        "id": "4QSAceNEzTmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        " Ans.  A Decision Tree is a supervised learning algorithm used for both classification and regression tasks. Think of it as a tree-like model: it starts at the root node (which contains all the data) and splits it into branches based on certain features or questions. At each internal node, the dataset is separated according to answers to those questions, progressively dividing into smaller subsets. This process continues until you reach a leaf node, which provides the final classification or output.\n",
        "\n",
        " In classification:\n",
        "    i) The decision tree separates data based on features (like age, income, etc.),\n",
        "    ii) Each leaf node represents a class label (for example, \"spam\" or \"not spam\"),\n",
        "    iii) The algorithm keeps partitioning until all items in a branch belong to the same category or another stopping criterion is met.\n",
        "\n",
        "\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Ans.  Gini Impurity and Entropy are two common ways to measure how mixed or impure the data is at each node of a decision tree.\n",
        "\n",
        "  i) Gini Impurity estimates how often a randomly chosen element from the node would be incorrectly classified if it were labeled according to the current class distribution. If all items at a node belong to one class, the impurity is zero (the node is pure); if classes are evenly mixed, impurity is higher.\n",
        "\n",
        "  ii) Entropy gauges the uncertainty or disorder in a node. Higher entropy means more unpredictability—i.e., classes are mixed. Lower entropy is more predictable or pure, meaning the data belongs mostly to one class.\n",
        "\n",
        " Impact on Tree Splits:\n",
        "\n",
        "   Both measures guide the decision tree in choosing the best point to split the data. The split that results in the lowest impurity (highest purity) is preferred, leading to simpler, more accurate decision branches. So, Gini and Entropy help the algorithm select features and split points that best separate the classes at each step, which improves the classification accuracy and makes the tree's predictions more reliable.\n",
        "\n",
        "\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Ans.  Difference Between Pre-Pruning and Post-Pruning in Decision Trees\n",
        "Pre-Pruning (Early Stopping):\n",
        "\n",
        "  i) This technique stops the growth of the decision tree early during the training process before it becomes too complex. It uses predefined conditions to halt splitting further, such as limiting the maximum depth or requiring a minimum number of samples to split a node. The goal is to prevent overfitting by keeping the tree simpler from the start.\n",
        "\n",
        "  ii) Post-Pruning: In contrast, post-pruning lets the tree grow fully without restrictions and then removes branches or nodes that do not significantly improve the model's accuracy. This simplification happens after the full tree is built, often using methods like cost-complexity pruning or reduced error pruning.\n",
        "\n",
        "Practical Advantages:\n",
        "\n",
        "  i) Pre-Pruning Advantage: It is computationally efficient since it stops building unnecessary branches early, saving training time, especially useful for large datasets.\n",
        "\n",
        "  ii) Post-Pruning Advantage: It often results in better model accuracy and generalization because it evaluates the fully grown tree before simplifying, reducing the risk of stopping too early and underfitting.\n",
        "\n",
        "\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Ans.  Information Gain is a concept used in decision trees to measure how well a feature (or attribute) helps to split the data into distinct classes. In simple terms, it tells us how much our uncertainty about the target class decreases if we know the value of a particular feature.\n",
        "\n",
        " When building a decision tree, the algorithm looks at each feature and calculates the information gain from splitting the data on that feature. The feature with the highest information gain is selected for the split at that node. This helps the tree create branches where the resulting groups are more pure (meaning most items in a group belong to the same class).\n",
        "\n",
        "Information gain is essential because:\n",
        "\n",
        "  i) It guides the tree to choose features that separate the classes most effectively at each step.\n",
        "\n",
        "  ii) Features that produce the largest reduction in impurity or uncertainty (i.e., higher information gain) lead to simpler, more accurate classification paths.\n",
        "\n",
        "  iii) By favoring splits that maximize information gain, the tree can more quickly homogeneously group data, reducing errors and overfitting.\n",
        "\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Ans. Common Real-World Applications of Decision Trees:\n",
        "\n",
        "  a) Healthcare: Used for disease diagnosis by analyzing symptoms and test results to classify conditions, aiding doctors in decision-making.\n",
        "\n",
        "  b) Finance: Employed for credit scoring, risk assessment, and fraud detection by evaluating financial histories and transaction patterns.\n",
        "\n",
        "  c) Education: Predict student performance or dropout risk based on attendance, grades, and study habits.\n",
        "\n",
        "  d) Marketing and Customer Segmentation: Classify customers by demographics and buying behavior for targeted marketing campaigns.\n",
        "\n",
        "  e) Manufacturing and Quality Control: Detect product defects based on sensor or production variables.\n",
        "\n",
        "  f) Retail and E-commerce: Build recommendation systems and manage inventory based on customer preferences and purchase data.\n",
        "\n",
        "  g) Agriculture: Predict crop yields and manage pest control using environmental data.\n",
        "\n",
        "\n",
        "Advantages of Decision Trees:\n",
        "\n",
        "   a) Easy to Understand and Interpret: They produce human-readable rules, making them ideal for explaining decisions to stakeholders.\n",
        "\n",
        "   b) Handles Both Numerical and Categorical Data: Versatile in terms of the types of data they can process.\n",
        "\n",
        "   c) Nonlinear Relationships: Can model complex decision boundaries without assuming linearity.\n",
        "\n",
        "   d) Require Little Data Preparation: They do not need normalization or scaling.\n",
        "\n",
        "Limitations of Decision Trees:\n",
        "\n",
        "   a) Overfitting: Trees can become overly complex, fitting noise in the training data.\n",
        "\n",
        "   b) Instability: Small changes in data can produce very different trees.\n",
        "\n",
        "   c) Bias Toward Features with More Levels: Features with many distinct values can dominate splits.\n",
        "\n",
        "   d) Poor Generalization on Some Datasets: May perform poorly compared to ensemble methods like random forests."
      ],
      "metadata": {
        "id": "IlrkFaDvzYbp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky-6BXH5zRVJ",
        "outputId": "b5fc2d02-35c6-47b2-dfdd-7e654a0b4d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.96\n",
            "Feature: sepal length (cm), Importance: 0.0215\n",
            "Feature: sepal width (cm), Importance: 0.0215\n",
            "Feature: petal length (cm), Importance: 0.0632\n",
            "Feature: petal width (cm), Importance: 0.8939\n"
          ]
        }
      ],
      "source": [
        "# 6)Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Train a Decision Tree Classifier using the Gini criterion\n",
        "# Print the model’s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Initialize Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=1)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Print feature importances\n",
        "features = iris.feature_names\n",
        "importances = clf.feature_importances_\n",
        "for feature, importance in zip(features, importances):\n",
        "    print(f'Feature: {feature}, Importance: {importance:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7) Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Train Decision Tree Classifier without depth limit (fully-grown tree)\n",
        "clf_full = DecisionTreeClassifier(random_state=1)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Train Decision Tree Classifier with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "print(f\"Accuracy of fully-grown tree: {accuracy_full:.2f}\")\n",
        "print(f\"Accuracy of tree with max_depth=3: {accuracy_limited:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEo2eZ9O5psz",
        "outputId": "37fb6e03-50d7-4de3-8559-760f1769fbac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully-grown tree: 0.96\n",
            "Accuracy of tree with max_depth=3: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8) Write a Python program to:\n",
        "# Load the Boston Housing Dataset\n",
        "# Train a Decision Tree Regressor\n",
        "# Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing Dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split dataset into training and testing parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Initialize Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=1)\n",
        "\n",
        "# Train the model\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "\n",
        "# Print feature importances\n",
        "features = housing.feature_names\n",
        "importances = regressor.feature_importances_\n",
        "for feature, importance in zip(features, importances):\n",
        "    print(f'Feature: {feature}, Importance: {importance:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZDHIKi46RBg",
        "outputId": "298df2fb-0464-4d07-a177-9b3d4cd9fca4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.4908\n",
            "Feature: MedInc, Importance: 0.5107\n",
            "Feature: HouseAge, Importance: 0.0507\n",
            "Feature: AveRooms, Importance: 0.0306\n",
            "Feature: AveBedrms, Importance: 0.0276\n",
            "Feature: Population, Importance: 0.0267\n",
            "Feature: AveOccup, Importance: 0.1384\n",
            "Feature: Latitude, Importance: 0.1088\n",
            "Feature: Longitude, Importance: 0.1064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9) Write a Python program to:\n",
        "# Load the Iris Dataset\n",
        "# Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "# Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=1)\n",
        "\n",
        "# Parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# GridSearchCV setup\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Predict using best estimator\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCPImCOS7LNo",
        "outputId": "3df9dcf5-95b6-44da-f7ab-eeb584579032"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Model Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you're working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "    ● Handle the missing values\n",
        "    ● Encode the categorical features\n",
        "    ● Train a Decision Tree model\n",
        "    ● Tune its hyperparameters\n",
        "    ● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "\n",
        "Ans.  Step-by-Step Machine Learning Workflow for Disease Prediction\n",
        "Let's break down how you'd handle this healthcare prediction task:\n",
        "\n",
        "a. Handle Missing Values:\n",
        "\n",
        "     i) Identify missing data: Check which features have missing entries and how many.\n",
        "     ii) Decide on imputation strategies:\n",
        "         For numerical variables, fill missing values with techniques such as mean, median, or model-based imputation.\n",
        "         For categorical variables, fill with the most frequent category or use a separate “missing” label.\n",
        "     iii) Consider the impact: Evaluate if the missingness carries meaning and if advanced techniques like Multiple Imputation are needed.\n",
        "\n",
        "b. Encode Categorical Features\n",
        "\n",
        "     i) Identify categorical columns.\n",
        "     ii) Choose encoding method based on feature type and cardinality:\n",
        "           For nominal categories with no order: use One-Hot Encoding.\n",
        "           For ordinal categories: use Ordinal Encoding.\n",
        "           For high-cardinality features: consider target encoding or embedding.\n",
        "\n",
        "c. Train a Decision Tree Model\n",
        "\n",
        "     i) Split the dataset into training and test sets to validate performance fairly.\n",
        "     ii) Initialize a Decision Tree Classifier; decision trees handle mixed data types well but require encoded input for categorical features.\n",
        "     iii) Train the model on the training data.\n",
        "\n",
        "d. Tune Hyperparameters\n",
        "\n",
        "     i) Identify important hyperparameters: max_depth, min_samples_split, min_samples_leaf, criterion (like Gini or Entropy).\n",
        "     ii) Use techniques like Grid Search or Randomized Search with cross-validation to find the best parameters without overfitting.\n",
        "\n",
        "e. Evaluate Performance\n",
        "\n",
        "     i) Use metrics suitable for classification: accuracy, precision, recall, F1-score, and especially AUC-ROC due to potential class imbalance.\n",
        "     ii) Analyze confusion matrix to understand types of errors.\n",
        "     iii) Consider validation with a separate test set or cross-validation for robustness.\n",
        "\n",
        "Business Value:\n",
        "\n",
        "     i) Early disease detection: The model helps identify patients at high risk quickly, aiding timely interventions.\n",
        "     ii) Resource allocation: Enables the healthcare company to better target diagnostic tests and treatments, optimizing cost and care delivery.\n",
        "     iii) Improved patient outcomes: Early prediction can reduce complications by allowing earlier treatment.\n",
        "     iv) Data-driven decisions: Provides insights into factors affecting disease presence, supporting clinical research and policy."
      ],
      "metadata": {
        "id": "pVW9Jf707pUe"
      }
    }
  ]
}